# TIL: 웹 애플리케이션과 LLM 래퍼 서비스 분리의 장점

## 문제 상황

> 프로젝트는 직접 LLM(Large Language Model) API를 호출하는 대신, 별도의 Node.js 기반 LLM 래퍼 서비스를 만들어 Glitch에 배포한 후, 그 URL을 환경변수로 받아와 사용.

### 구조에 대한 의문
> "왜 서비스를 분리해서 만든거야? 그냥 spring으로 다 만들면 되는거 아니야?"


### 분리된 서비스의 성능 이점에 대한 의문
> "LLM 래퍼 서비스의 부하가 높아져도 웹 앱의 성능에 직접적인 영향이 적음 -> 어짜피 LLM서비스 답변이 늦어지면 웹앱도 페이지 느리게 뜨잖아?"

### 추가 -> LLM 서비스 호출 방식에 대한 혼란
> "이게 배포한 url을 환경변수로 받아서 LLM 래퍼 서비스를 이용하는건데 정확히 로직이 어떻게 이루어지는지 모르겠어. url만 가져왔는데 어떻게 그 서비스를 사용할 수 있는거지?"

## 문제 해결

### 아키텍처 설계의 이해 (LLM 래퍼 서비스와 웹 애플리케이션 통신 방식)

Spring 웹 애플리케이션이 외부 LLM 래퍼 서비스와 통신하는 방식은 `AIRepository` 클래스에 잘 구현되어 있었습니다:

```java
@Repository
public class AIRepository {
    // 환경 변수에서 API URL을 가져옵니다
    String baseUrl = dotenv.get("AI_API_URL");

    public <T> T useModel(String model, AIRequest param, Class<T> responseType) throws Exception {
        // baseUrl(환경변수에서 가져온 URL)과 모델명을 조합해 전체 API URL을 만듭니다
        HttpRequest request = HttpRequest.newBuilder()
                .uri(URI.create("%s/%s".formatted(baseUrl, model)))
                .POST(HttpRequest.BodyPublishers.ofString(
                        objectMapper.writeValueAsString(param)))
                .headers("Content-Type", "application/json")
                .build();
        
        // 요청을 보내고 응답을 받아옵니다
        HttpResponse<String> response = httpClient.send(
                request, HttpResponse.BodyHandlers.ofString()
        );
        
        // 응답을 적절한 DTO 객체로 변환해 반환합니다
        return objectMapper.readValue(response.body(), responseType);
    }
}
```

환경변수로 받아온 URL(예: "https://mytype-llm-wrapper.glitch.me")에 모델명(llama, deepseek, flux)을 붙여 API 엔드포인트를 구성하고, HTTP 요청을 보내는 방식이었습니다.

### LLM 래퍼 서비스와 웹 애플리케이션 분리의 장점
- 역할 분담:

    -  LLM 래퍼 서비스: AI 모델과 대화하는 역할만 담당
    - 웹 애플리케이션: 웹사이트 디자인, 기능, 사용자와의 소통만 담당
    > 각자 잘하는 것에 집중해서 효율적으로 개발 가능

- 기술 스택 최적화:

    - LLM 래퍼 서비스에게는 AI 연결에 편리하고 가벼운 도구 (예: Node.js)를 사용
    - 웹사이트 전문가에게는 웹사이트 개발에 강력한 도구 (예: Spring)를 사용
    - 각 역할에 맞는 최적의 기술을 활용 가능

- 확장성과 재사용성:

    - 만들어 놓은 AI 전문가는 다른 여러 웹사이트에서 재활용 가능
    - 웹사이트에서 사용하는 AI를 필요에 따라 다른 똑똑한 AI로 쉽게 교체 가능

- 독립적인 성장 (스케일링):

    - LLM 래퍼 서비스가 바빠지면 LLM서비스 개개수만 따로 늘려서 처리 능력을 키울 수 있음 (수평적 스케일링)
    - 웹사이트 트래픽이 늘어도 웹사이트 전문가만 서버를 늘리는 방식으로 독립적으로 확장 가능
    - LLM 래퍼 서비스에 문제가 생겨도 웹사이트의 다른 기능은 정상적으로 작동 가능

- 보안 강화:

    - **중요한 정보 (API 키)**를 웹사이트와 분리된 AI 전문가에게만 관리하게 해서 더 안전하게 보호 가능

## 느낀점 및 보완점
### 느낀점

1. 시스템 아키텍처의 중요성
- 처음에는 "왜 이렇게 복잡하게 만들었지?"라고 생각했지만, 분석하면서 각 부분이 독립적으로 발전하고 유지보수할 수 있는 이런 구조가 장기적으로 더 효율적임을 깨달음.
2. 기술 선택의 이유
- 모든 기술 선택에는 이유가 있다. Node.js와 Spring을 함께 활용한 것은 각각의 장점을 최대화하기 위한 전략적 선택이었음.
3. 확장성 고려 
- 당장은 단순해 보이는 구현도 미래의 확장성을 고려하면 더 복잡한 아키텍처가 필요할 수 있음. 처음부터 확장성을 고려한 설계는 장기적으로 큰 이점을 가져옴.

### 보완점

1. **비동기 처리 도입**
- 현재 구현은 AI 응답을 동기적으로 기다리는 구조로, 비동기 처리를 도입하면 사용자 경험을 크게 향상시킬 수 있을 것
2. **타임아웃 및 폴백 전략**
- AI 서비스 응답 지연이나 오류 발생 시 대응하는 타임아웃 설정과 폴백 전략을 구현하면 더 견고한 시스템이 될 것임.
3. **캐싱 도입**
- 자주 요청되는 AI 응답을 캐싱하여 응답 시간을 단축하고 외부 API 호출을 줄일 수 있음음
4. **병렬 처리 최적화**
- 여러 AI 모델 호출을 병렬로 처리하여 전체 페이지 로딩 시간을 단축할 수 있을 것임임